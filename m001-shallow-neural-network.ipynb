{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks\n",
    "\n",
    "## Network Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization: Single Input Vector Case\n",
    "\n",
    "Shallow, 2-layer network with three input features, $\\textbf{x} = x_1$, $x_2$, $x_3$, and a single hidden layer with four nodes/neurons, $\\textbf{a}_1$, $\\textbf{a}_2$, $\\textbf{a}_3$, $\\textbf{a}_4$\n",
    "\n",
    "- $n_{h}^{[l]}$ refers to the number of hidden units in the $l^{th}$ layer  \n",
    "- $\\textbf{w}_{i}^{[l]}$ denotes the weight vector of the $i^{th}$ node in the $l^{th}$ layer  \n",
    "- $W^{[l]} \\in \\mathbb{R}^{n_{h}^{[l+1]} \\; \\times \\; n_{h}^{[l-1]}} \\equiv W^{[l]} \\in \\mathbb{R}^{\\textrm{number of units in next layer} \\; \\times \\; \\textrm{number of units in previous layer}}$, which is the weight matrix of the $l^{th}$ layer \n",
    "- $\\textbf{x} \\in \\mathbb{R}^{n_x \\times 1}$, also be written as $\\textbf{a}^{[0]}$, refers to the input vector, and $\\textbf{x} \\mapsto a^{[2]} = \\hat{y}$  \n",
    "\n",
    "First Node:\n",
    "$$ z_{1}^{[1]} = w_{1}^{[1]T} \\textbf{x} + b_{1}^{[1]} \\;\\;\\; ;\\quad a_{1}^{[1]} = \\sigma(z_{1}^{[1]}) \\tag{1, 2} $$\n",
    "\n",
    "Second Node:\n",
    "$$ z_{2}^{[1]} = w_{2}^{[1]T} \\textbf{x} + b_{2}^{[1]} \\;\\;\\; ;\\quad a_{2}^{[1]} = \\sigma(z_{2}^{[1]}) \\tag{3, 4} $$\n",
    "and so on\n",
    "\n",
    "$$\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\n",
    "$$\n",
    "\n",
    "\n",
    "Take $W$ and multiply it with the features:\n",
    "\n",
    "$$\n",
    "\\textbf{z}^{[1]}_{4 \\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z_{1}^{[1]}\\\\ \n",
    "z_{2}^{[1]}\\\\\n",
    "z_{3}^{[1]}\\\\\n",
    "z_{4}^{[1]}\\\\\n",
    "\\end{bmatrix}_{4 \\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "— w_{1}^{[1]T} —  \\\\ \n",
    "— w_{2}^{[1]T} — \\\\\n",
    "— w_{3}^{[1]T} — \\\\\n",
    "— w_{4}^{[1]T} — \\\\\n",
    "\\end{bmatrix}_{4 \\times 3}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\ \n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "\\end{bmatrix}_{3 \\times 1}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1}^{[1]} \\\\ \n",
    "b_{2}^{[1]} \\\\\n",
    "b_{3}^{[1]} \\\\\n",
    "b_{4}^{[1]} \\\\\n",
    "\\end{bmatrix}_{4 \\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{1}^{[1]T} \\textbf{x}+ b_{1}^{[1]} \\\\ \n",
    "w_{2}^{[1]T} \\textbf{x}+ b_{2}^{[1]} \\\\\n",
    "w_{3}^{[1]T} \\textbf{x}+ b_{3}^{[1]} \\\\\n",
    "w_{4}^{[1]T} \\textbf{x}+ b_{4}^{[1]} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "W^{[1]} \\textbf{x} + \\textbf{b}^{[1]}\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{a}^{[1]}_{4 \\times 1} = \\sigma(\\textbf{z}^{[1]}) \\tag{6}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "z^{[2]}_{1 \\times 1} = W^{[2]}_{1 \\times 4} \\; \\textbf{a}^{[1]}_{4 \\times 1} + b^{[2]}_{1 \\times 1} \\\\\n",
    "a^{[2]}_{1 \\times 1} = \\sigma(z^{[2]})\n",
    "$$\n",
    "\n",
    "to produce output $\\hat{y} = a^{[2]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing multiple $z$ as a vector\n",
    "\n",
    "Given input $X \\in \\mathbb{R}^{n_x \\times m} := \\textbf{a}^{[0]}$, where\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{x}^{(1)} & \\textbf{x}^{(2)} & \\ldots & \\textbf{x}^{(m)} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}_{n_x \\times m}\n",
    "$$\n",
    "map $X \\longmapsto \\hat{\\textbf{y}}$, i.e.\n",
    "$$\\textbf{x}^{(1)} \\mapsto \\hat{y}^{(1)} := a^{[2](1)}$$\n",
    "$$\\textbf{x}^{(2)} \\mapsto \\hat{y}^{(2)} := a^{[2](2)}$$\n",
    "$$\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\textbf{x}^{(m)} \\mapsto \\hat{y}^{(m)} := a^{[2](m)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unvectorized Implementation\n",
    "`for i=1 to m:`\n",
    "\n",
    "&emsp; $\\textbf{z}^{[1](i)} = W^{[1]} \\textbf{x}^{(i)} + b^{[1]}$\n",
    "\n",
    "&emsp; $\\textbf{a}^{[1](i)} = \\sigma(\\textbf{z}^{[1][i]})$\n",
    "   \n",
    "&emsp; $\\textbf{z}^{[2](i)} = W^{[2]} \\textbf{a}^{[1][i]} + b^{[2]}$\n",
    "   \n",
    "&emsp; $\\textbf{a}^{[2](i)} = \\sigma(\\textbf{z}^{[2][i]})$\n",
    "\n",
    "The primary goal is to remove the `for` loop that runs over all the training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation\n",
    "\n",
    "Consider:\n",
    "\n",
    "$$\n",
    "Z^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{z}^{[1](1)} & \\textbf{z}^{[1](2)} & \\ldots & \\textbf{z}^{[1](m)} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}_{4 \\times m} \\; \\textrm{where} \\; \\textbf{z}^{[1](i)} \\in \\mathbb{R}^{4 \\times 1}\n",
    "$$\n",
    "\n",
    "$Z^{[1]}$ is thus simply $m$ number of $\\textbf{z}^{[1]}$'s stacked horizontally.  \n",
    "The weight matrix $W^{[1]}$ stays the same shape, $(4, 3)$, and $X$ is of the shape of $(3, m)$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$Z^{[1]}_{4 \\times m} = W^{[1]} X + b^{[1]} $$\n",
    "$$A^{[1]}_{4 \\times m} = \\sigma(Z^{[1]}) $$\n",
    "$$Z^{[2]}_{1 \\times m} = W^{[2]}_{1 \\times 4} \\; A^{[1]} + b^{[2]} $$\n",
    "$$A^{[2]}_{1 \\times m} = \\sigma(Z^{[2]}) $$\n",
    "\n",
    "$A^{[1]}$ is similar to $Z^{[1]}$: $m$ number of $\\textbf{a}^{[1]}$'s stacked horizontally.  \n",
    "Note that the vertical indices(rows) correspond to different nodes in the network: the first node in the first layer $\\rightarrow$ top-left corner of the $Z$ and $A$ matrices."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}