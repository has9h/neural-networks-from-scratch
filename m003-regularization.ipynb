{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2-Regularization\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "**Note: Incomplete lecture. Linked are the lectures that need to be perused to complete the sections**\n",
    "\n",
    "$\\require{cancel}$\n",
    "### Regularized Cost Function:\n",
    "\n",
    "- $\\textbf{w} \\in \\mathbb{R}^{n_x}$\n",
    "- $b \\in \\mathbb{R}$\n",
    "\n",
    "Adding the regularization parameter, $\\lambda$ to the cost function:\n",
    "\n",
    "$$J(\\textbf{w}, b) = \\frac{1}{m} \\sum^{m}_{i=1} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\lVert \\textbf{w} \\rVert^{2}_{2} \\quad + \\underbrace{\\cancel{\\frac{\\lambda}{2m} b^{2}}}_{omit}$$\n",
    "where $\\lVert \\textbf{w} \\rVert^{2}_{2} = \\sum^{n_x}_{j=1} \\textbf{w}^{2}_{j} = \\textbf{w}^{T} \\textbf{w}$, which is the squared Euclidean norm of the parameter vector $\\textbf{w}$, also known as the L2-norm.\n",
    "\n",
    "Given that $\\textbf{w}$ is a high-parameter vector, $b$ is simply a single parameter number, and can be omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Regularized Cost Function:\n",
    "\n",
    "$$J(\\textbf{w}^{[1]}, \\textbf{b}^{[1]}, \\ldots ,\\textbf{w}^{[L]}, \\textbf{b}^{[L]}) = \\frac{1}{m} \\sum^{m}_{i=1} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum^{L}_{l=1} \\lVert \\textbf{w} \\rVert^{2}_{2} \\quad + \\underbrace{\\cancel{\\frac{\\lambda}{2m} b^{2}}}_{omit}$$\n",
    "\n",
    "where $\\lVert \\textbf{w} \\rVert^{2}_{2}$ is the Frobenius Norm:\n",
    "\n",
    "$\\lVert w^{[l]} \\rVert^{2} = \\sum_{i=1}^{n[l]} \\sum_{j=1}^{n[l-1]}(w^{[l]}_{i,j})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative Update:\n",
    "\n",
    "$$dW^{l} = \\text{backprop_term} + \\frac{\\lambda}{m}W^{[l]}$$\n",
    "\n",
    "### Gradient Descent:\n",
    "\n",
    "The weight update rule now becomes:  \n",
    "\n",
    "$$W^{[l]} := W^{[l]} - \\alpha \\; dW^{[l]}$$\n",
    "\n",
    "This can be rearranged to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W^{[l]} :&= W^{[l]} - \\alpha \\; [\\text{backprop_term} + \\frac{\\lambda}{m}W^{[l]}] \\\\\n",
    "&= W^{[l]} - \\frac{\\alpha \\; \\lambda}{m} \\; W^{[l]} - \\alpha \\; \\text{backprop_term} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which shows that $W^{[l]}$ gets multiplied by $(1 - \\frac{\\alpha \\; \\lambda}{m})$. This is why L2-regularization is also called weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization\n",
    "\n",
    "## Inverted Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "This section needs to be clarified and looked into deeper\n",
    "\n",
    "[Link to lecture](https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization)\n",
    "\n",
    "Considering a single layer in a network, with l=3:\n",
    "\n",
    "Let `keep_prob` be the probability that a given hidden unit will be kept.  \n",
    "Let `d3` be the dropout dropout layer for the 3rd layer, described by:\n",
    "\n",
    "```\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1])\n",
    "a3 = np.multiply(a3, d3)\n",
    "a3 /= keep_prob\n",
    "```\n",
    "`a3` is scaled up by `keep_prob`.\n",
    "\n",
    "Dropout should not be applied in testing time. It is also interesting to note that using dropout causes the cost function to not be well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regularization Methods\n",
    "\n",
    "1. Increasing training set: Data Augmentation\n",
    "2. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Given an input $\\textbf{x}$ with $2$ features, $x_1, x_2$:\n",
    "\n",
    "Subtract the mean:\n",
    "\n",
    "$$\\mathbf{\\mu} = \\frac{1}{m} \\sum^{m}_{i=1} x^{i}$$\n",
    "$$\\textbf{x} := \\textbf{x} - \\mathbf{\\mu}$$ \n",
    "\n",
    "Normalize the variance:\n",
    "\n",
    "$$\\mathbf{\\sigma^{2}} = \\frac{1}{m} \\sum^{m}_{i=1} {x^{i}}^{2}$$\n",
    "\n",
    "This means $\\mathbf{\\sigma^{2}}$ is a vector with the variances of each of the features. Finally, take each example, and\n",
    "\n",
    "$$\\textbf{x} \\; /= \\mathbf{\\sigma}$$\n",
    "\n",
    "*Then use the same $\\sigma$ and $\\mu$ to normalize the test set as well*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients\n",
    "\n",
    "[Link to lecture](https://www.coursera.org/learn/deep-neural-network/lecture/C9iQO/vanishing-exploding-gradients)\n",
    "\n",
    "Consider a very deep neural network with the same number of neurons for each hidden layer, with 2 input features and a linear activation function, i.e. $g(z) = z$.  \n",
    "The output variable $\\hat{y}$ can then be written as\n",
    "\n",
    "$$\\hat{y} = W^{[l]}\\;W^{[l-1]}\\;W^{[l-2]} \\ldots \\underbrace{W^{[3]} \\underbrace{W^{[2]} \\underbrace{W^{[1]} \\textbf{x}}_{\\textbf{a}^{[1]} = g(\\textbf{z}^{[1]}) = \\textbf{z}^{[1]}}}_{\\textbf{a}^{[2]} = g(\\textbf{z}^{[2]})}}_{\\ldots}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization for Deep Networks\n",
    "\n",
    "[Link to lecture](https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks)\n",
    "\n",
    "The following holds for a single neuron, and assuming a RelU activation function:\n",
    "\n",
    "Set var($w_i$) = $\\frac{2}{n}$  \n",
    "\n",
    "$W^{[l]} = \\text{np.random.randn(<shape>) * np.sqrt}(\\frac{2}{n^{[l-1]}})$\n",
    "\n",
    "**Other variants**\n",
    "\n",
    "Xavier Initialization: `tanh` $\\sqrt{\\frac{1}{n^{[l-1]}}}$\n",
    "\n",
    "*The variance term may also be a hyperparameter to tune*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "[Link to lecture](https://www.coursera.org/learn/deep-neural-network/lecture/htA0l/gradient-checking)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}