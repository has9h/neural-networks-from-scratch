{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Gradient Descent\n",
    "\n",
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $m$ denotes the number of examples in the dataset  \n",
    "- $n_x$ denotes the input size, or the number of features, per example  \n",
    "- $n_y$ denotes the output size  \n",
    "\n",
    "- $X \\in \\mathbb{R}^{n_x \\times m}$ is the input matrix\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{x}^{1} & \\textbf{x}^{2} & \\ldots & \\textbf{x}^{m} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $Y \\in \\mathbb{R}^{n_y \\times m}$ is the label matrix\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{y}^{1} & \\textbf{y}^{2} & \\ldots & \\textbf{y}^{m} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $\\textbf{x}^{(i)} \\in \\mathbb{R}^{n_x}$ is input feature for the $i^{th}$ example\n",
    "- $\\textbf{y}^{(i)} \\in \\mathbb{R}^{n_y \\times m}$ is the output label for the $i^{th}$ example  \n",
    "- $\\hat{\\textbf{y}} \\in \\mathbb{R}^{n_y}$ is the predicted output vector. Also denoted with $\\textbf{a}^{(i)}$  \n",
    "- $\\textbf{w}$ refers to the weight vector, where $w \\in \\mathbb{R}^{1 \\times n_x}$\n",
    "\n",
    "- $J(\\hat{\\textbf{y}}, \\textbf{y})$ denotes the cost function \n",
    "\n",
    "Coding Notation\n",
    "\n",
    "- $dw_1 := \\frac{\\partial J(\\textbf{w}, b)}{\\partial w_1}$  \n",
    "\n",
    "- $dw_2 := \\frac{\\partial J(\\textbf{w}, b)}{\\partial w_2}$  \n",
    "\n",
    "- $db := \\frac{\\partial J(\\textbf{w}, b)}{\\partial b}$  \n",
    "\n",
    "The following refer to a single input:\n",
    "\n",
    "$$z = w^T \\textbf{x} + b  \\tag{1} $$\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}} \\tag{2} $$\n",
    "$$\\hat{y} = a = \\sigma(z) \\tag{3} $$\n",
    "$$\\mathcal{L}(a, y) = -(y \\log(a) + (1 - y) \\log(1 - a)) \\tag{4} $\n",
    "$$J(\\textbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y)) \\tag{5} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "\n",
    "### Basic Implementation: Over `m` example training sets\n",
    "\n",
    "```\n",
    "J = 0; dw_1 = 0; dw_2 = 0; db = 0\n",
    "\n",
    "// loop over all training examples\n",
    "for i = 1 to m\n",
    "    z_i = w.T * x_i + b\n",
    "    a_i = sigma(z_i)\n",
    "    J += -[y_i * log(a_i) + (1 - y_i) * log(1 - a_i)]\n",
    "    dz_i = a_i - y_i\n",
    "\n",
    "    // loop over all features\n",
    "    for each feature\n",
    "    dw_1 += x_1i * dz_i\n",
    "    dw_2 += x_2i * dz_i\n",
    "    db += dz_i\n",
    "J /= m\n",
    "dw_1 /= m\n",
    "dw_2 /= m\n",
    "db /= m\n",
    "\n",
    "w_1 = w_1 - alpha * dw_1\n",
    "w_2 = w_2 - alpha * dw_2\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation\n",
    "\n",
    "*$z$ now becomes $\\textbf{z}$, $\\hat{y} = a$ becomes $\\hat{\\textbf{y}} = \\textbf{a}$, and $\\textbf{x}$ becomes $X$*\n",
    "\n",
    "Rewrite $z^{(1)}, z^{(2)}, \\; \\ldots , \\; z^{(m)} $ (individual $z$ for multiple examples) as\n",
    "\n",
    "$$\n",
    "Z =\n",
    "\\begin{bmatrix}\n",
    "\\textbf{z}^{(1)} & \\textbf{z}^{(2)} & \\ldots & \\textbf{z}^{(m)}\n",
    "\\end{bmatrix}\n",
    "= w^T X + b, \\tag{6}\n",
    "$$\n",
    "\n",
    "$a^{(1)}, a^{(2)}, \\; \\ldots , \\; a^{(m)}$ as\n",
    "\n",
    "$$\n",
    "A = \\sigma(Z) = \\sigma(w^T X + b) =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(\\textbf{z})^{(1)} & \\sigma(\\textbf{z})^{(2)} & \\ldots & \\sigma(\\textbf{z})^{(1)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\textbf{a}^{(1)} & \\textbf{a}^{(2)} & \\ldots & \\textbf{a}^{(m)}\n",
    "\\end{bmatrix} \\tag{7}\n",
    "$$\n",
    "\n",
    "#### Expanding $w^T X$\n",
    "\n",
    "Recall that $X \\in \\mathbb{R}^{n_x \\times m}$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w_1 & w_2 & \\ldots & w_m\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{x}^{1} & \\textbf{x}^{2} & \\ldots & \\textbf{x}^{m} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_1 x^{1}_1 & w_2 x^{2}_1 & \\ldots & w_m x^{m}_1 \\\\\n",
    "w_2 x^{1}_2 & w_2 x^{2}_2 & \\ldots & w_m x^{m}_2 \\\\\n",
    ". & . & \\ldots & . \\\\\n",
    "w_m x^{1}_{n_x} & w_2 x^{2}_{n_x} & \\ldots & w_m x^{m}_{n_x}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Derivation of $\\frac{d\\mathcal{L}}{dz}$\n",
    "\n",
    "First note that $dz := \\frac{d\\mathcal{L}}{dz}$  \n",
    "By the chain rule\n",
    "\n",
    "$$\\frac{d\\mathcal{L}}{dz} = \\frac{d\\mathcal{L}}{da} \\times \\frac{da}{dz} $$\n",
    "\n",
    "As  \n",
    "\n",
    "$$\\mathcal{L}(\\textbf{a}, \\textbf{y}) = -(\\textbf{y} \\log(\\textbf{a}) + (1 - \\textbf{y}) \\log(1 - \\textbf{a})) $$\n",
    "$$\\frac{d\\mathcal{L}}{da} \\; = \\; -y \\times \\frac{1}{a} \\; - \\; (1 - y) \\times \\frac{1}{1 - a} \\times -1 $$\n",
    "$$\\frac{d\\mathcal{L}}{da} = \\frac{-y}{a} + \\frac{1 - y}{1 - a} = \\frac{a - y}{a(1 - a)} \\tag{8} $$\n",
    "$$\\frac{da}{dz} = \\frac{d}{dz} \\sigma({\\textbf(z)}) = \\sigma({\\textbf(z)}) \\times (1 - \\sigma({\\textbf(z)})) = a(1 - a) \\tag{9} $$\n",
    "$$\\frac{d\\mathcal{L}}{dz} = \\frac{a - y}{a(1 - a)} \\times a(1 - a) = a - y $$\n",
    "\n",
    "Hence \n",
    "\n",
    "$$dz = a - y \\tag{10} $$\n",
    "\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "$$\n",
    "dZ =\n",
    "\\begin{bmatrix}\n",
    "\\textbf{dz}^{(1)} & \\textbf{dz}^{(2)} & \\ldots & \\textbf{dz}^{(m)}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\textbf{a}^{(1)} - \\textbf{y}^{(1)} & \\textbf{a}^{(2)} - \\textbf{y}^{(2)} & \\ldots & \\textbf{a}^{(m)} - \\textbf{y}^{(m)}\n",
    "\\end{bmatrix} = \n",
    "A - Y\n",
    "$$\n",
    "$$db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)}) = \\frac{1}{m} \\sum_{i=1}^{m} d\\textbf{z}^{(i)} \\tag{11} $$\n",
    "$$dw = \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T = \\frac{1}{m} X dZ^T \\tag{12} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Removing the inner loop\n",
    "\n",
    "```\n",
    "J = 0; dw = np.zeros(n_x, 1); db = 0\n",
    "\n",
    "// loop over all training examples\n",
    "for i = 1 to m\n",
    "    z_i = w.T * x_i + b\n",
    "    a_i = sigma(z_i)\n",
    "    J += -[y_i * log(a_i) + (1 - y_i) * log(1 - a_i)]\n",
    "    dz_i = a_i - y_i\n",
    "\n",
    "    dw += x_i * dz_i\n",
    "    db += dz_i\n",
    "J /= m\n",
    "dw /= m\n",
    "db /= m\n",
    "\n",
    "w_1 = w_1 - alpha * dw_1\n",
    "w_2 = w_2 - alpha * dw_2\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Removing the outer loop\n",
    "\n",
    "```\n",
    "J = 0; dw = np.zeros(n_x, 1); db = 0\n",
    "\n",
    "Z = np.dot(w.T, X) + b\n",
    "A = sigma(Z)\n",
    "J += -[y_i * log(a_i) + (1 - y_i) * log(1 - a_i)]\n",
    "dZ = A - Y\n",
    "\n",
    "dw = 1/m * X * dZ.T\n",
    "db = 1/m np.sum(dZ)\n",
    "\n",
    "J /= m\n",
    "\n",
    "w = w - alpha * dw\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}