{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Neural Networks\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "With **$n_x = n^{[0]}$ input features**, **$n^{[1]}$ hidden units**, and **$n^{[2]} = 1$ output units**, we have\n",
    "\n",
    "- $W^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times n_x}$ dimensional matrix  \n",
    "- $\\textbf{b}^{[1]}$ is an $n^{[1]}$ dimensional vector, or $\\in \\mathbb{R}^{n^{[1]} \\times 1}$ dimensional matrix\n",
    "- $W^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times n^{[1]}}$\n",
    "- $\\textbf{b}^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1}$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$$J(W^{[1]}, \\; \\textbf{b}^{[1]}, \\; W^{[2]}, \\; \\textbf{b}^{[2]}) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{\\textbf{y}}, \\; \\textbf{y}) \\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture Representation\n",
    "\n",
    "*Note: Recall that $\\textbf{x} = a^{[0]}$ and that $n_x = n^{[0]}$*\n",
    "\n",
    "$$\n",
    "\\underbrace{\\textbf{x},\\; W^{[1]},\\; \\textbf{b}^{[1]}}_{input}\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\textbf{z}^{[1]} = \\overbrace{\\underbrace{W^{[1]}}_{n^{[1]} \\times n^{[0]}} \\; \\underbrace{\\textbf{x}}_{n^{[0]} \\times 1}}^{n^{[1]} \\times 1} + \\textbf{b}^{[1]}\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\textbf{a}^{[1]} = g^{[1]}(\\textbf{z}^{[1]})\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\underbrace{W^{[2]},\\; \\textbf{b}^{[2]}}_{input}\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\textbf{z}^{[2]} = \\overbrace{\\underbrace{W^{[2]}}_{n^{[2]} \\times n^{[1]}} \\; \\underbrace{\\textbf{a}^{[1]}}_{n^{[1]} \\times 1}}^{n^{[2]} \\times 1} + \\textbf{b}^{[2]}\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\textbf{a}^{[2]} = g^{[1]}(\\textbf{z}^{[2]})\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\underbrace{\\mathcal{L}(\\textbf{a}^{[2]}, \\textbf{y})}_{output}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}, \\; \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{b}^{[1]}}}_{derivatives}\n",
    "\\quad \\longleftarrow \\quad\n",
    "d \\textbf{z}^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{z}^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{a}^{[1]}} \\cdot \\frac{d \\textbf{a}^{[1]}}{d \\textbf{z}^{[1]}}\n",
    "\\quad \\longleftarrow \\quad\n",
    "d \\textbf{a}^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{a}^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{z}^{[2]}} \\cdot \\frac{d \\textbf{z}^{[2]}}{d \\textbf{a}^{[1]}}\n",
    "\\quad \\longleftarrow \\quad\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}, \\; \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{b}^{[2]}}}_{derivatives}\n",
    "\\quad \\longleftarrow \\quad\n",
    "d \\textbf{z}^{[2]} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{z}^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{a}^{[2]}} \\cdot \\frac{d \\textbf{a}^{[2]}}{d \\textbf{z}^{[2]}}\n",
    "\\quad \\longleftarrow \\quad\n",
    "\\frac{d \\textbf{a}^{[2]}}{d \\textbf{z}^{[2]}}\n",
    "\\quad \\longleftarrow \\quad\n",
    "d \\textbf{a}^{[2]} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{a}^{[2]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensional Analysis\n",
    "\n",
    "Note that corresponding units and their derivatives have the same dimensions:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{c}\n",
    "\\textbf{z}^{[1]}, \\; d \\textbf{z}^{[1]}, \\; \\textbf{a}^{[1]}, \\; d \\textbf{a}^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times 1} \\\\\n",
    "\\textbf{z}^{[2]}, \\; d \\textbf{z}^{[2]}, \\; \\textbf{a}^{[2]}, \\; d \\textbf{a}^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Because $\\textbf{b}^{[1]}, \\; \\textbf{b}^{[2]}$ represents scalars per neuron, they are also of the dimensions $(n^{[1]} \\times 1)$ and $(n^{[2]} \\times 1)$ respectively.  \n",
    "In cases where $X$ is passed as an input, the bias units are broadcast over all $m$ examples before being added to $W^{[1]} \\; X$ and $W^{[2]} \\; X$, i.e. $(n^{[1]} \\times m)$ and $(n^{[2]} \\times m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "For a vectorized implementation over `m` training examples:\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[1]} &= W^{[1]} X + \\textbf{b}^{[1]} \\tag{2} \\\\[1em]\n",
    "A^{[1]} &= g^{[1]}(Z^{[1]}) \\tag{3} \\\\[1em]\n",
    "Z^{[2]} &= W^{[2]} \\; A^{[1]} + \\textbf{b}^{[2]} \\tag{4} \\\\[1em]\n",
    "A^{[2]} &= g^{[2]}(Z^{[2]}) \\tag{5} \\\\[1em]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Recall that, for a single input $\\textbf{x}$, we start with the chain rule, working backwards\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{da} = \\frac{-y}{a} + \\frac{1 - y}{1 - a} = \\frac{a - y}{a(1 - a)}\n",
    "$$\n",
    "\n",
    "Similarly, as $\\textbf{a}^{[2]}$ simply represents the activation unit in the second layer\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} = \\frac{-y}{a^{[2]}} + \\frac{1 - y}{1 - a^{[2]}} = \\frac{a^{[2]} - y}{a^{[2]}(1 - a^{[2]})}\n",
    "$$  \n",
    "Following a similar reasoning for all other units\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} &= a^{[2]} - y = d \\textbf{z}^{[2]} \\\\[1em]\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}} &= \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\times \\frac{d z^{[2]}}{d b^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} = d \\textbf{z}^{[2]} \\tag{6} \\\\[1em]\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} &= \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\times \\frac{d z^{[2]}}{d W^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\times \\textbf{a}^{[1]} = d \\textbf{z}^{[2]} \\cdot \\textbf{a}^{[1]T} \\tag{7} \\\\[1em]\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} &= W^{[2]T} \\cdot d\\textbf{z}^{[2]}  = d \\textbf{a}^{[1]} \\\\[1em]\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} &= \\underbrace{(W^{[2]T} \\cdot d\\textbf{z}^{[2]})}_{n^{[1]} \\times 1} * \\underbrace{g^{[1]'}(z^{[1]})}_{n^{[1]} \\times 1} = d \\textbf{z}^{[1]} \\\\[1em]\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} &= \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\times \\frac{d z^{[1]}}{d b^{[1]}} = d \\textbf{z}^{[1]} \\tag{8} \\\\[1em]\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}} &= \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\times \\frac{d z^{[1]}}{d W^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\times \\textbf{a}^{[0]} = d \\textbf{z}^{[1]} \\cdot \\textbf{a}^{[0]T} \\tag{9}\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "***Note: The \\* operator indicates element-wise product***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The seemingly faulty $T$ in our $\\textbf{a}^{[1]T}$ and $W^{[2]T}$\n",
    "\n",
    "From $(3)$, without the transpose, we would have the following\n",
    "\n",
    "$$\\underbrace{d \\textbf{z}^{[2]}}_{n^{[2]} \\times 1} \\cdot \\underbrace{\\textbf{a}^{[1]}}_{n^{[1]} \\times 1},$$\n",
    "which is not a valid operation. To ensure that the derivatives of each node gets multiplied with the corresponding activation units from the previous layer's nodes, $a^{[1]}$ is transposed, resulting in the derivative weight matrix of size $(n^{[2]} \\times n^{[1]})$ for layer $[2]$.\n",
    "\n",
    "A similar reasoning applies when computing $d \\textbf{a}^{[1]}$ and $d \\textbf{z}^{[1]}$\n",
    "\n",
    "$$\\underbrace{W^{[2]T}}_{n^{[1]} \\times n^{[2]}} \\cdot \\underbrace{d\\textbf{z}^{[2]}}_{n^{[2]} \\times 1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Backpropagation\n",
    "\n",
    "Stacking all of the training examples horizontally(column-wise), allows the same pattern to follow through\n",
    "$$\n",
    "\\begin{align}\n",
    "dZ^{[2]} &= A^{[2]} - Y \\tag{10}^{*} \\\\[1em]\n",
    "dW^{[2]} &= \\frac{1}{m}dZ^{[2]}\\;A^{[1]T} \\tag{11} \\\\[1em]\n",
    "d \\textbf{b}^{[2]} &= \\frac{1}{m} \\sum dZ^{[2]^{**}} \\tag{12} \\\\[1em]\n",
    "dZ^{[1]} &= \\underbrace{W^{[2]T} \\; dZ^{[2]}}_{n^{[1]} \\;\\times\\; m} * \\underbrace{g^{[1]']} \\; Z^{[1]}}_{n^{[1]} \\;\\times\\; m} \\tag{13} \\\\[1em]\n",
    "dW^{[1]} &= \\frac{1}{m}dZ^{[1]}X^T \\tag{10} \\\\[1em]\n",
    "\\underbrace{d \\textbf{b}^{[1]}}_{n^{[1]} \\;\\times\\; m} &= \\frac{1}{m}\\sum dZ^{[1]^{***}} \\tag{14} \\\\[1em]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$^{*} (6)$ Assumes binary classification  \n",
    "$^{**} (8)$ can be written as `(1/m) * np.sum(dZ2, axis=1, keepdims=True)`. This retains the shape $(n^{[2]}, 1)$.  \n",
    "$^{***} (11)$ can be written as `(1/m) * np.sum(dZ1, axis=1, keepdims=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Repeat {\n",
    "    \n",
    "&emsp; Compute predictions: $\\hat{y}^{(i)} \\; \\forall i$  \n",
    "\n",
    "&emsp; $dW^{[1]} = \\frac{\\partial J}{\\partial W^{[1]}}, \\;\\; db^{[1]} = \\frac{\\partial J}{\\partial b^{[1]}} , \\; \\cdots $\n",
    "\n",
    "&emsp; $W^{[1]} = W^{[1]} - \\alpha \\; dW^{[1]}$\n",
    "\n",
    "&emsp; $\\textbf{b}^{[1]} = \\textbf{b}^{[1]} - \\alpha \\; d\\textbf{b}^{[1]} $\n",
    "\n",
    "&emsp; $\\vdots$\n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}