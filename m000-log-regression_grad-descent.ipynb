{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Gradient Descent\n",
    "\n",
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $m$ denotes the number of examples in the dataset  \n",
    "- $n_x$ denotes the input size, or the number of features, per example  \n",
    "- $n_y$ denotes the output size  \n",
    "\n",
    "- $X \\in \\mathbb{R}^{n_x \\times m}$ is the input matrix\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{x}^{1} & \\textbf{x}^{2} & \\ldots & \\textbf{x}^{m} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}_{n_x \\times m}\n",
    "$$\n",
    "\n",
    "- $Y \\in \\mathbb{R}^{n_y \\times m}$ is the label matrix\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{y}^{1} & \\textbf{y}^{2} & \\ldots & \\textbf{y}^{m} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}_{n_y \\times m}\n",
    "$$\n",
    "\n",
    "- $\\textbf{x}^{(i)} \\in \\mathbb{R}^{n_x}$ is input feature for the $i^{th}$ example\n",
    "- $\\textbf{y}^{(i)} \\in \\mathbb{R}^{n_y \\times m}$ is the output label for the $i^{th}$ example  \n",
    "- $\\hat{\\textbf{y}} \\in \\mathbb{R}^{n_y}$ is the predicted output vector. Also denoted with $\\textbf{a}^{(i)}$  \n",
    "- $\\textbf{w}$ refers to the weight vector, where $w \\in \\mathbb{R}^{1 \\times n_x}$\n",
    "\n",
    "- $J(\\hat{\\textbf{y}}, \\textbf{y})$ denotes the cost function \n",
    "\n",
    "Coding Notation\n",
    "\n",
    "- $dw_1 := \\frac{\\partial J(\\textbf{w}, b)}{\\partial w_1}$  \n",
    "\n",
    "- $dw_2 := \\frac{\\partial J(\\textbf{w}, b)}{\\partial w_2}$  \n",
    "\n",
    "- $db := \\frac{\\partial J(\\textbf{w}, b)}{\\partial b}$  \n",
    "\n",
    "The following refer to a single input:\n",
    "\n",
    "$$z = w^T \\textbf{x} + b  \\tag{1} $$\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}} \\tag{2} $$\n",
    "$$\\hat{y} = a = \\sigma(z) \\tag{3} $$\n",
    "$$\\mathcal{L}(a, y) = -(y \\log(a) + (1 - y) \\log(1 - a)) \\tag{4} $$\n",
    "$$J(\\textbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y)) \\tag{5} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "\n",
    "### Basic Implementation: Over `m` example training sets\n",
    "\n",
    "```\n",
    "J = 0; dw_1 = 0; dw_2 = 0; db = 0\n",
    "\n",
    "// loop over all training examples\n",
    "for i = 1 to m\n",
    "    z_i = w.T * x_i + b\n",
    "    a_i = sigma(z_i)\n",
    "    J += -[y_i * log(a_i) + (1 - y_i) * log(1 - a_i)]\n",
    "    dz_i = a_i - y_i\n",
    "\n",
    "    // loop over all features\n",
    "    for each feature\n",
    "    dw_1 += x_1i * dz_i\n",
    "    dw_2 += x_2i * dz_i\n",
    "    db += dz_i\n",
    "J /= m\n",
    "dw_1 /= m\n",
    "dw_2 /= m\n",
    "db /= m\n",
    "\n",
    "w_1 = w_1 - alpha * dw_1\n",
    "w_2 = w_2 - alpha * dw_2\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation\n",
    "\n",
    "*$z$ now becomes $\\textbf{z}$, $\\hat{y} = a$ becomes $\\hat{\\textbf{y}} = \\textbf{a}$, and $\\textbf{x}$ becomes $X$*\n",
    "\n",
    "Rewrite $z^{(1)}, z^{(2)}, \\; \\ldots , \\; z^{(m)} $ (individual $z$ for multiple examples) as\n",
    "\n",
    "$$\n",
    "Z =\n",
    "\\begin{bmatrix}\n",
    "\\textbf{z}^{(1)} & \\textbf{z}^{(2)} & \\ldots & \\textbf{z}^{(m)}\n",
    "\\end{bmatrix}\n",
    "= w^T X + b, \\tag{6}\n",
    "$$\n",
    "\n",
    "$a^{(1)}, a^{(2)}, \\; \\ldots , \\; a^{(m)}$ as\n",
    "\n",
    "$$\n",
    "A = \\sigma(Z) = \\sigma(w^T X + b) =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(\\textbf{z}^{(1)}) & \\sigma(\\textbf{z}^{(2)}) & \\ldots & \\sigma(\\textbf{z}^{(m)})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\textbf{a}^{(1)} & \\textbf{a}^{(2)} & \\ldots & \\textbf{a}^{(m)}\n",
    "\\end{bmatrix} \\tag{7}\n",
    "$$\n",
    "\n",
    "#### Expanding $w^T X$\n",
    "\n",
    "Recall that $X \\in \\mathbb{R}^{n_x \\times m}$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w_1 & w_2 & \\ldots & w_m\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\textbf{x}^{1} & \\textbf{x}^{2} & \\ldots & \\textbf{x}^{m} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_1 x^{1}_1 & w_2 x^{2}_1 & \\ldots & w_m x^{m}_1 \\\\\n",
    "w_2 x^{1}_2 & w_2 x^{2}_2 & \\ldots & w_m x^{m}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_m x^{1}_{n_x} & w_2 x^{2}_{n_x} & \\ldots & w_m x^{m}_{n_x}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Derivation of $\\frac{d\\mathcal{L}}{dz}$\n",
    "\n",
    "First note that $dz := \\frac{d\\mathcal{L}}{dz}$  \n",
    "By the chain rule\n",
    "\n",
    "$$\\frac{d\\mathcal{L}}{dz} = \\frac{d\\mathcal{L}}{da} \\times \\frac{da}{dz} $$\n",
    "\n",
    "As  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\textbf{a}, \\textbf{y}) &= -(\\textbf{y} \\log(\\textbf{a}) + (1 - \\textbf{y}) \\log(1 - \\textbf{a})) \\\\[1em]\n",
    "\\frac{d\\mathcal{L}}{da} \\; &= \\; -y \\times \\frac{1}{a} \\; - \\; (1 - y) \\times \\frac{1}{1 - a} \\times -1 \\\\[1em]\n",
    "\\frac{d\\mathcal{L}}{da} &= \\frac{-y}{a} + \\frac{1 - y}{1 - a} = \\frac{a - y}{a(1 - a)} \\tag{8} \\\\[1em]\n",
    "\\frac{da}{dz} &= \\frac{d}{dz} \\{\\sigma({\\textbf{z}})\\} = \\sigma({\\textbf{z}}) \\times (1 - \\sigma({\\textbf{z}})) = a(1 - a) \\tag{9} \\\\[1em]\n",
    "\\frac{d\\mathcal{L}}{dz} &= \\frac{a - y}{a(1 - a)} \\times a(1 - a) = a - y\n",
    "\\end{align}\n",
    "$$\n",
    "Hence \n",
    "\n",
    "$$dz = a - y \\tag{10} $$\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "$$\n",
    "dZ =\n",
    "\\begin{bmatrix}\n",
    "\\textbf{dz}^{(1)} & \\textbf{dz}^{(2)} & \\ldots & \\textbf{dz}^{(m)}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\textbf{a}^{(1)} - \\textbf{y}^{(1)} & \\textbf{a}^{(2)} - \\textbf{y}^{(2)} & \\ldots & \\textbf{a}^{(m)} - \\textbf{y}^{(m)}\n",
    "\\end{bmatrix} = \n",
    "A - Y\n",
    "$$\n",
    "\\begin{align}\n",
    "db &= \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)}) = \\frac{1}{m} \\sum_{i=1}^{m} d\\textbf{z}^{(i)} \\tag{11} \\\\[1em]\n",
    "dw &= \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T = \\frac{1}{m} X dZ^T \\tag{12} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Removing the inner loop\n",
    "\n",
    "```\n",
    "J = 0; dw = np.zeros(n_x, 1); db = 0\n",
    "\n",
    "// loop over all training examples\n",
    "for i = 1 to m\n",
    "    z_i = w.T * x_i + b\n",
    "    a_i = sigma(z_i)\n",
    "    J += -[y_i * log(a_i) + (1 - y_i) * log(1 - a_i)]\n",
    "    dz_i = a_i - y_i\n",
    "\n",
    "    dw += x_i * dz_i\n",
    "    db += dz_i\n",
    "J /= m\n",
    "dw /= m\n",
    "db /= m\n",
    "\n",
    "w_1 = w_1 - alpha * dw_1\n",
    "w_2 = w_2 - alpha * dw_2\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Removing the outer loop\n",
    "\n",
    "```\n",
    "J = 0; dw = np.zeros(n_x, 1); db = 0\n",
    "\n",
    "Z = np.dot(w.T, X) + b\n",
    "A = sigma(Z)\n",
    "J += -[y_i * log(a_i) + (1 - y_i) * log(1 - a_i)]\n",
    "dZ = A - Y\n",
    "\n",
    "dw = 1/m * X * dZ.T\n",
    "db = 1/m np.sum(dZ)\n",
    "\n",
    "J /= m\n",
    "\n",
    "w = w - alpha * dw\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Forward\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -1/m * (np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)))\n",
    "    \n",
    "    # Backprop\n",
    "    dw = (1/m) * (np.dot(X, (A - Y).T))\n",
    "    db = (1/m) * (np.sum(A - Y))\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0][i] <= 0.5:\n",
    "            Y_prediction[0][i] = 0\n",
    "        else:\n",
    "            Y_prediction[0][i] = 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code snippets are non-functional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost=False):\n",
    "\n",
    "    w, b = np.zeros((X_train.shape[0], 1)), 0\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=True)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-FUNCTIONAL\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-FUNCTIONAL\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}